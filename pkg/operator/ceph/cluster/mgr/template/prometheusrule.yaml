apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: rook-prometheus
    role: alert-rules
  name: prometheus-ceph-rules
  namespace: rook-ceph
spec:
  groups:
  - name: ceph.rules
    rules:
    - expr: |
        kube_node_status_condition{condition="Ready",job="kube-state-metrics",status="true"} * on (node) group_right() max(label_replace(ceph_disk_occupation{job="rook-ceph-mgr"},"node","$1","exported_instance","(.*)")) by (node, namespace)
      record: cluster:ceph_node_down:join_kube
    - expr: |
        avg(topk by (ceph_daemon) (1, label_replace(label_replace(ceph_disk_occupation{job="rook-ceph-mgr"}, "instance", "$1", "exported_instance", "(.*)"), "device", "$1", "device", "/dev/(.*)")) * on(instance, device) group_right(ceph_daemon) topk by (instance,device) (1,(irate(node_disk_read_time_seconds_total[1m]) + irate(node_disk_write_time_seconds_total[1m]) / (clamp_min(irate(node_disk_reads_completed_total[1m]), 1) + irate(node_disk_writes_completed_total[1m])))))
      record: cluster:ceph_disk_latency:join_ceph_node_disk_irate1m
  - name: telemeter.rules
    rules:
    - expr: |
        count(ceph_osd_metadata{job="rook-ceph-mgr"})
      record: job:ceph_osd_metadata:count
    - expr: |
        count(kube_persistentvolume_info * on (storageclass)  group_left(provisioner) kube_storageclass_info {provisioner=~"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)"})
      record: job:kube_pv:count
    - expr: |
        sum(ceph_pool_rd{job="rook-ceph-mgr"}+ ceph_pool_wr{job="rook-ceph-mgr"})
      record: job:ceph_pools_iops:total
    - expr: |
        sum(ceph_pool_rd_bytes{job="rook-ceph-mgr"}+ ceph_pool_wr_bytes{job="rook-ceph-mgr"})
      record: job:ceph_pools_iops_bytes:total
    - expr: |
        count(count(ceph_mon_metadata{job="rook-ceph-mgr"} or ceph_osd_metadata{job="rook-ceph-mgr"} or ceph_rgw_metadata{job="rook-ceph-mgr"} or ceph_mds_metadata{job="rook-ceph-mgr"} or ceph_mgr_metadata{job="rook-ceph-mgr"}) by(ceph_version))
      record: job:ceph_versions_running:count
  - name: ceph-mgr-status
    rules:
    {{- if not .CephMgrIsAbsent.Disabled }}
    - alert: CephMgrIsAbsent
      annotations:
        description: Ceph Manager has disappeared from Prometheus target discovery.
        message: Storage metrics collector service not available anymore.
        severity_level: {{ .CephMgrIsAbsent.SeverityLevel }}
        storage_type: ceph
      expr: |
        label_replace((up{job="rook-ceph-mgr"} == 0 or absent(up{job="rook-ceph-mgr"})), "namespace", "{{ .CephMgrIsAbsent.Namespace }}", "", "")
      for: {{ .CephMgrIsAbsent.For }}
      labels:
        severity: {{ .CephMgrIsAbsent.Severity }}
    {{- end }}
    {{- if not .CephMgrIsMissingReplicas.Disabled }}
    - alert: CephMgrIsMissingReplicas
      annotations:
        description: Ceph Manager is missing replicas.
        message: Storage metrics collector service doesn't have required no of replicas.
        severity_level: {{ .CephMgrIsMissingReplicas.SeverityLevel }}
        storage_type: ceph
      expr: |
        sum(kube_deployment_spec_replicas{deployment=~"rook-ceph-mgr-.*"}) by (namespace) < 1
      for: {{ .CephMgrIsMissingReplicas.For }}
      labels:
        severity: {{ .CephMgrIsMissingReplicas.Severity }}
    {{- end }}
  - name: ceph-mds-status
    rules:
    {{- if not .CephMdsMissingReplicas.Disabled }}
    - alert: CephMdsMissingReplicas
      annotations:
        description: Minimum required replicas for storage metadata service not available. Might affect the working of storage cluster.
        message: Insufficient replicas for storage metadata service.
        severity_level: {{ .CephMdsMissingReplicas.SeverityLevel }}
        storage_type: ceph
      expr: |
        sum(ceph_mds_metadata{job="rook-ceph-mgr"} == 1) by (namespace) < 2
      for: {{ .CephMdsMissingReplicas.For }}
      labels:
        severity: {{ .CephMdsMissingReplicas.Severity }}
    {{- end }}
  - name: quorum-alert.rules
    rules:
    {{- if not .CephMonQuorumAtRisk.Disabled }}
    - alert: CephMonQuorumAtRisk
      annotations:
        description: Storage cluster quorum is low. Contact Support.
        message: Storage quorum at risk
        severity_level: {{ .CephMonQuorumAtRisk.SeverityLevel }}
        storage_type: ceph
      expr: |
        count(ceph_mon_quorum_status{job="rook-ceph-mgr"} == 1) by (namespace) <= (floor(count(ceph_mon_metadata{job="rook-ceph-mgr"}) by (namespace) / 2) + 1)
      for: {{ .CephMonQuorumAtRisk.For }}
      labels:
        severity: {{ .CephMonQuorumAtRisk.Severity }}
    {{- end }}
    {{- if not .CephMonQuorumLost.Disabled }}
    - alert: CephMonQuorumLost
      annotations:
        description: Storage cluster quorum is lost. Contact Support.
        message: Storage quorum is lost
        severity_level: {{ .CephMonQuorumLost.SeverityLevel }}
        storage_type: ceph
      expr: |
        count(kube_pod_status_phase{pod=~"rook-ceph-mon-.*", phase=~"Running|running"} == 1) by (namespace) < 2
      for: {{ .CephMonQuorumLost.For }}
      labels:
        severity: {{ .CephMonQuorumLost.Severity }}
    {{- end }}
    {{- if not .CephMonHighNumberOfLeaderChanges.Disabled }}
    - alert: CephMonHighNumberOfLeaderChanges
      annotations:
        description: Ceph Monitor `{{`{{`}} $labels.ceph_daemon {{`}}`}}` on host `{{`{{`}} $labels.hostname {{`}}`}}` has seen `{{`{{`}} $value {{`}}`}}` | printf "%.2f" }} leader changes per minute recently.
        message: Storage Cluster has seen many leader changes recently.
        severity_level: {{ .CephMonHighNumberOfLeaderChanges.SeverityLevel }}
        storage_type: ceph
      expr: |
        (ceph_mon_metadata{job="rook-ceph-mgr"} * on (ceph_daemon) group_left() (rate(ceph_mon_num_elections{job="rook-ceph-mgr"}[5m]) * 60)) > 0.{{ .CephMonHighNumberOfLeaderChanges.Limit }}
      for: {{ .CephMonHighNumberOfLeaderChanges.For }}
      labels:
        severity: {{ .CephMonHighNumberOfLeaderChanges.Severity }}
    {{- end }}
  - name: ceph-node-alert.rules
    rules:
    {{- if not .CephNodeDown.Disabled }}
    - alert: CephNodeDown
      annotations:
        description: Storage node `{{`{{`}} $labels.node {{`}}`}}` went down. Please check the node
          immediately.
        message: Storage node `{{`{{`}} $labels.node {{`}}`}}` went down
        severity_level: {{ .CephNodeDown.SeverityLevel }}
        storage_type: ceph
      expr: |
        cluster:ceph_node_down:join_kube == 0
      for: {{ .CephNodeDown.For }}
      labels:
        severity: {{ .CephNodeDown.Severity }}
    {{- end }}
  - name: osd-alert.rules
    rules:
    {{- if not .CephOSDCriticallyFull.Disabled }}
    - alert: CephOSDCriticallyFull
      annotations:
        description: Utilization of storage device `{{`{{`}} $labels.ceph_daemon {{`}}`}}` of device_class
          type `{{`{{`}} $labels.device_class {{`}}`}}` has crossed {{ .CephOSDCriticallyFull.Limit }}% on host `{{`{{`}} $labels.hostname {{`}}`}}`. Immediately free up some space or add capacity of type `{{`{{`}} $labels.device_class {{`}}`}}`.
        message: Back-end storage device is critically full.
        severity_level: {{ .CephOSDCriticallyFull.SeverityLevel }}
        storage_type: ceph
      expr: |
        (ceph_osd_metadata * on (ceph_daemon) group_right(device_class) (ceph_osd_stat_bytes_used / ceph_osd_stat_bytes)) >= 0.{{ .CephOSDCriticallyFull.Limit }}
      for: {{ .CephOSDCriticallyFull.For }}
      labels:
        severity: {{ .CephOSDCriticallyFull.Severity }}
    {{- end }}
    {{- if not .CephOSDFlapping.Disabled }}
    - alert: CephOSDFlapping
      annotations:
        description: Storage daemon `{{`{{`}} $labels.ceph_daemon {{`}}`}}` has restarted {{ .CephOSDFlapping.Limit }} times
          in last {{ .CephOSDFlapping.OsdUpRate }} . Please check the pod events or ceph status to find out
          the cause.
        message: Ceph storage osd flapping.
        severity_level: {{ .CephOSDFlapping.SeverityLevel }}
        storage_type: ceph
      expr: |
        changes(ceph_osd_up[{{ .CephOSDFlapping.OsdUpRate }}]) >= {{ .CephOSDFlapping.Limit }}
      for: {{ .CephOSDFlapping.For }}
      labels:
        severity: {{ .CephOSDFlapping.Severity }}
    {{- end }}
    {{- if not .CephOSDNearFull.Disabled }}
    - alert: CephOSDNearFull
      annotations:
        description: Utilization of storage device `{{`{{`}} $labels.ceph_daemon {{`}}`}}` of device_class
          type `{{`{{`}} $labels.device_class {{`}}`}}` has crossed {{ .CephOSDNearFull.Limit }}% on host `{{`{{`}} $labels.hostname {{`}}`}}`. Immediately free up some space or add capacity of type `{{`{{`}} $labels.device_class {{`}}`}}.
        message: Back-end storage device is nearing full.
        severity_level: {{ .CephOSDNearFull.SeverityLevel }}
        storage_type: ceph
      expr: |
        (ceph_osd_metadata * on (ceph_daemon) group_right(device_class) (ceph_osd_stat_bytes_used / ceph_osd_stat_bytes)) >= 0.{{ .CephOSDNearFull.Limit }}
      for: {{ .CephOSDNearFull.For }}
      labels:
        severity: {{ .CephOSDNearFull.Severity }}
    {{- end }}
    {{- if not .CephOSDDiskNotResponding.Disabled }}
    - alert: CephOSDDiskNotResponding
      annotations:
        description: Disk device `{{`{{`}} $labels.device {{`}}`}}` not responding, on host `{{`{{`}} $labels.host {{`}}`}}`.
        message: Disk not responding
        severity_level: {{ .CephOSDDiskNotResponding.SeverityLevel }}
        storage_type: ceph
      expr: |
        label_replace((ceph_osd_in == 1 and ceph_osd_up == 0),"disk","$1","ceph_daemon","osd.(.*)") + on(ceph_daemon) group_left(host, device) label_replace(ceph_disk_occupation,"host","$1","exported_instance","(.*)")
      for: {{ .CephOSDDiskNotResponding.For }}
      labels:
        severity: {{ .CephOSDDiskNotResponding.Severity }}
    {{- end }}
    {{- if not .CephOSDDiskUnavailable.Disabled }}
    - alert: CephOSDDiskUnavailable
      annotations:
        description: Disk device `{{`{{`}} $labels.device {{`}}`}}` not accessible on host `{{`{{`}} $labels.host {{`}}`}}`.
        message: Disk not accessible
        severity_level: {{ .CephOSDDiskUnavailable.SeverityLevel }}
        storage_type: ceph
      expr: |
        label_replace((ceph_osd_in == 0 and ceph_osd_up == 0),"disk","$1","ceph_daemon","osd.(.*)") + on(ceph_daemon) group_left(host, device) label_replace(ceph_disk_occupation,"host","$1","exported_instance","(.*)")
      for: {{ .CephOSDDiskUnavailable.For }}
      labels:
        severity: {{ .CephOSDDiskUnavailable.Severity }}
    {{- end }}
    {{- if not .CephOSDSlowOps.Disabled }}
    - alert: CephOSDSlowOps
      annotations:
        description: '`{{`{{`}} $value {{`}}`}}` Ceph OSD requests are taking too long to process.
          Please check ceph status to find out the cause.'
        message: OSD requests are taking too long to process.
        severity_level: {{ .CephOSDSlowOps.SeverityLevel }}
        storage_type: ceph
      expr: |
        ceph_healthcheck_slow_ops > 0
      for: {{ .CephOSDSlowOps.For }}
      labels:
        severity: {{ .CephOSDSlowOps.Severity }}
    {{- end }}
    {{- if not .CephDataRecoveryTakingTooLong.Disabled }}
    - alert: CephDataRecoveryTakingTooLong
      annotations:
        description: Data recovery has been active for too long. Contact Support.
        message: Data recovery is slow
        severity_level: {{ .CephDataRecoveryTakingTooLong.SeverityLevel }}
        storage_type: ceph
      expr: |
        ceph_pg_undersized > 0
      for: {{ .CephDataRecoveryTakingTooLong.For }}
      labels:
        severity: {{ .CephDataRecoveryTakingTooLong.Severity }}
    {{- end }}
    {{- if not .CephPGRepairTakingTooLong.Disabled }}
    - alert: CephPGRepairTakingTooLong
      annotations:
        description: Self heal operations taking too long. Contact Support.
        message: Self heal problems detected
        severity_level: {{ .CephPGRepairTakingTooLong.SeverityLevel }}
        storage_type: ceph
      expr: |
        ceph_pg_inconsistent > 0
      for: {{ .CephPGRepairTakingTooLong.For }}
      labels:
        severity: {{ .CephPGRepairTakingTooLong.Severity }}
    {{- end }}
  - name: persistent-volume-alert.rules
    rules:
    {{- if not .PersistentVolumeUsageNearFull.Disabled }}
    - alert: PersistentVolumeUsageNearFull
      annotations:
        description: PVC `{{`{{`}} $labels.persistentvolumeclaim {{`}}`}}` utilization has crossed
          {{ .PersistentVolumeUsageNearFull.Limit }}%. Free up some space or expand the PVC.
        message: PVC `{{`{{`}} $labels.persistentvolumeclaim {{`}}`}}` is nearing full. Data deletion
          or PVC expansion is required.
        severity_level: {{ .PersistentVolumeUsageNearFull.SeverityLevel }}
        storage_type: ceph
      expr: |
        (kubelet_volume_stats_used_bytes * on (namespace,persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info * on (storageclass)  group_left(provisioner) kube_storageclass_info {provisioner=~"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)"})) / (kubelet_volume_stats_capacity_bytes * on (namespace,persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info * on (storageclass)  group_left(provisioner) kube_storageclass_info {provisioner=~"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)"})) > 0.{{ .PersistentVolumeUsageNearFull.Limit }}
      for: {{ .PersistentVolumeUsageNearFull.For }}
      labels:
        severity: {{ .PersistentVolumeUsageNearFull.Severity }}
    {{- end }}
    {{- if not .PersistentVolumeUsageCritical.Disabled }}
    - alert: PersistentVolumeUsageCritical
      annotations:
        description: PVC `{{`{{`}} $labels.persistentvolumeclaim {{`}}`}}` utilization has crossed
          {{ .PersistentVolumeUsageCritical.Limit }}%. Free up some space or expand the PVC immediately.
        message: PVC `{{`{{`}} $labels.persistentvolumeclaim {{`}}`}}` is critically full. Data
          deletion or PVC expansion is required.
        severity_level: {{ .PersistentVolumeUsageCritical.SeverityLevel }}
        storage_type: ceph
      expr: |
        (kubelet_volume_stats_used_bytes * on (namespace,persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info * on (storageclass)  group_left(provisioner) kube_storageclass_info {provisioner=~"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)"})) / (kubelet_volume_stats_capacity_bytes * on (namespace,persistentvolumeclaim) group_left(storageclass, provisioner) (kube_persistentvolumeclaim_info * on (storageclass)  group_left(provisioner) kube_storageclass_info {provisioner=~"(.*rbd.csi.ceph.com)|(.*cephfs.csi.ceph.com)"})) > 0.{{ .PersistentVolumeUsageCritical.Limit }}
      for: {{ .PersistentVolumeUsageCritical.For }}
      labels:
        severity: {{ .PersistentVolumeUsageCritical.Severity }}
    {{- end }}
  - name: cluster-state-alert.rules
    rules:
    {{- if not .CephClusterErrorState.Disabled }}
    - alert: CephClusterErrorState
      annotations:
        description: Storage cluster is in error state for more than {{ .CephClusterErrorState.For }}.
        message: Storage cluster is in error state
        severity_level: {{ .CephClusterErrorState.SeverityLevel }}
        storage_type: ceph
      expr: |
        ceph_health_status{job="rook-ceph-mgr"} > 1
      for: {{ .CephClusterErrorState.For }}
      labels:
        severity: {{ .CephClusterErrorState.Severity }}
    {{- end }}
    {{- if not .CephClusterWarningState.Disabled }}
    - alert: CephClusterWarningState
      annotations:
        description: Storage cluster is in warning state for more than {{ .CephClusterWarningState.For }}.
        message: Storage cluster is in degraded state
        severity_level: {{ .CephClusterWarningState.SeverityLevel }}
        storage_type: ceph
      expr: |
        ceph_health_status{job="rook-ceph-mgr"} == 1
      for: {{ .CephClusterWarningState.For }}
      labels:
        severity: {{ .CephClusterWarningState.Severity }}
    {{- end }}
    {{- if not .CephOSDVersionMismatch.Disabled }}
    - alert: CephOSDVersionMismatch
      annotations:
        description: There are `{{`{{`}} $value {{`}}`}}` different versions of Ceph OSD components
          running.
        message: There are multiple versions of storage services running.
        severity_level: {{ .CephOSDVersionMismatch.SeverityLevel }}
        storage_type: ceph
      expr: |
        count(count(ceph_osd_metadata{job="rook-ceph-mgr"}) by (ceph_version, namespace)) by (ceph_version, namespace) > 1
      for: {{ .CephOSDVersionMismatch.For }}
      labels:
        severity: {{ .CephOSDVersionMismatch.Severity }}
    {{- end }}
    {{- if not .CephMonVersionMismatch.Disabled }}
    - alert: CephMonVersionMismatch
      annotations:
        description: There are `{{`{{`}} $value {{`}}`}}` different versions of Ceph Mon components
          running.
        message: There are multiple versions of storage services running.
        severity_level: {{ .CephMonVersionMismatch.SeverityLevel }}
        storage_type: ceph
      expr: |
        count(count(ceph_mon_metadata{job="rook-ceph-mgr", ceph_version != ""}) by (ceph_version)) > 1
      for: {{ .CephMonVersionMismatch.For }}
      labels:
        severity: {{ .CephMonVersionMismatch.Severity }}
    {{- end }}
  - name: cluster-utilization-alert.rules
    rules:
    {{- if not .CephClusterNearFull.Disabled }}
    - alert: CephClusterNearFull
      annotations:
        description: Storage cluster utilization has crossed {{ .CephClusterNearFull.Limit }}% and will become read-only
          at 85%. Free up some space or expand the storage cluster.
        message: Storage cluster is nearing full. Data deletion or cluster expansion
          is required.
        severity_level: {{ .CephClusterNearFull.SeverityLevel }}
        storage_type: ceph
      expr: |
        ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes > 0.{{ .CephClusterNearFull.Limit }}
      for: {{ .CephClusterNearFull.For }}
      labels:
        severity: {{ .CephClusterNearFull.Severity }}
    {{- end }}
    {{- if not .CephClusterCriticallyFull.Disabled }}
    - alert: CephClusterCriticallyFull
      annotations:
        description: Storage cluster utilization has crossed {{ .CephClusterCriticallyFull.Limit }}% and will become read-only
          at 85%. Free up some space or expand the storage cluster immediately.
        message: Storage cluster is critically full and needs immediate data deletion
          or cluster expansion.
        severity_level: {{ .CephClusterCriticallyFull.SeverityLevel }}
        storage_type: ceph
      expr: |
        ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes > 0.{{ .CephClusterCriticallyFull.Limit }}
      for: {{ .CephClusterCriticallyFull.For }}
      labels:
        severity: {{ .CephClusterCriticallyFull.Severity }}
    {{- end }}
    {{- if not .CephClusterReadOnly.Disabled }}
    - alert: CephClusterReadOnly
      annotations:
        description: Storage cluster utilization has crossed 85% and will become read-only
          now. Free up some space or expand the storage cluster immediately.
        message: Storage cluster is read-only now and needs immediate data deletion
          or cluster expansion.
        severity_level: {{ .CephClusterReadOnly.SeverityLevel }}
        storage_type: ceph
      expr: |
        ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes >= 0.{{ .CephClusterReadOnly.Limit }}
      for: {{ .CephClusterReadOnly.For }}
      labels:
        severity: {{ .CephClusterReadOnly.Severity }}
    {{- end }}
  - name: pool-quota.rules
    rules:
    {{- if not .CephPoolQuotaBytesNearExhaustion.Disabled }}
    - alert: CephPoolQuotaBytesNearExhaustion
      annotations:
        description: Storage pool `{{`{{`}} $labels.name {{`}}`}}` quota usage has crossed {{ .CephPoolQuotaBytesNearExhaustion.Limit }}%.
        message: Storage pool quota(bytes) is near exhaustion.
        severity_level: {{ .CephPoolQuotaBytesNearExhaustion.SeverityLevel }}
        storage_type: ceph
      expr: |
        (ceph_pool_stored_raw * on (pool_id) group_left(name)ceph_pool_metadata) / ((ceph_pool_quota_bytes * on (pool_id) group_left(name)ceph_pool_metadata) > 0) > 0.{{ .CephPoolQuotaBytesNearExhaustion.Limit }}
      for: {{ .CephPoolQuotaBytesNearExhaustion.For }}
      labels:
        severity: {{ .CephPoolQuotaBytesNearExhaustion.Severity }}
    {{- end }}
    {{- if not .CephPoolQuotaBytesCriticallyExhausted.Disabled }}
    - alert: CephPoolQuotaBytesCriticallyExhausted
      annotations:
        description: Storage pool `{{`{{`}} $labels.name {{`}}`}}` quota usage has crossed {{ .CephPoolQuotaBytesCriticallyExhausted.Limit }}%.
        message: Storage pool quota(bytes) is critically exhausted.
        severity_level: {{ .CephPoolQuotaBytesCriticallyExhausted.SeverityLevel }}
        storage_type: ceph
      expr: |
        (ceph_pool_stored_raw * on (pool_id) group_left(name)ceph_pool_metadata) / ((ceph_pool_quota_bytes * on (pool_id) group_left(name)ceph_pool_metadata) > 0) > 0.{{ .CephPoolQuotaBytesCriticallyExhausted.Limit }}
      for: {{ .CephPoolQuotaBytesCriticallyExhausted.For }}
      labels:
        severity: {{ .CephPoolQuotaBytesCriticallyExhausted.Severity }}
    {{- end }}
