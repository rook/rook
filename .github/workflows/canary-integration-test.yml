name: Canary integration tests
on:
  pull_request:
    paths:
    - '**.go'
    - '**.sh'
    - '**.py'
    - 'build/**'
    - 'tests/**'
    - 'images/**'
    - '.github/workflows/**'

jobs:
  canary:
    runs-on: ubuntu-18.04
    steps:
    - name: checkout
      uses: actions/checkout@v2

    - name: setup golang
      uses: actions/setup-go@v2
      with:
        go-version: 1.16

    - name: setup minikube
      uses: manusa/actions-setup-minikube@v2.3.1
      with:
        minikube version: 'v1.13.1'
        kubernetes version: 'v1.19.2'
        start args: --memory 6g --cpus=2
        github token: ${{ secrets.GITHUB_TOKEN }}

    - name: check k8s cluster status
      run: |
        kubectl cluster-info
        kubectl get pods -n kube-system

    - name: use local disk and create partitions for osds
      run: |
        BLOCK=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)
        BLOCK_DATA_PART=${BLOCK}1
        sudo dmsetup version||true
        sudo swapoff --all --verbose
        sudo umount /mnt
        # search for the device since it keeps changing between sda and sdb
        sudo wipefs --all --force "$BLOCK_DATA_PART"
        sudo lsblk
        tests/scripts/create-bluestore-partitions.sh --disk "$BLOCK" --osd-count 2
        sudo lsblk

    - name: build rook
      run: |
        # set VERSION to a dummy value since Jenkins normally sets it for us. Do this to make Helm happy and not fail with "Error: Invalid Semantic Version"
        GOPATH=$(go env GOPATH) make clean && make -j$nproc IMAGES='ceph' VERSION=0 build
        docker images
        docker tag $(docker images|awk '/build-/ {print $1}') rook/ceph:v1.6.6


    - name: validate-yaml
      run: |
        cd cluster/examples/kubernetes/ceph/
        kubectl create -f crds.yaml -f common.yaml
        # skipping folders and some yamls that are only for openshift.
        kubectl create  $(ls -I scc.yaml -I "*-openshift.yaml"  -I "*.sh" -I "*.py" -p | grep -v / | awk ' { print " -f " $1 } ') --dry-run

    - name: deploy rook
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/operator.yaml
        sed -i "s|#deviceFilter:|deviceFilter: $(lsblk|awk '/14G/ {print $1}'| head -1)|g" cluster/examples/kubernetes/ceph/cluster-test.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/cluster-test.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/object-test.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/pool-test.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/filesystem-test.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/rbdmirror.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/nfs-test.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/toolbox.yaml

    - name: wait for prepare pod
      run: |
        timeout 300 sh -c 'until kubectl -n rook-ceph logs -f $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}'); do sleep 5; done'

    - name: wait for ceph to be ready
      run: |
        mkdir test
        tests/scripts/validate_cluster.sh all 2
        kubectl -n rook-ceph get pods

    - name: test external script create-external-cluster-resources.py
      run: |
        toolbox=$(kubectl get pod -l app=rook-ceph-tools -n rook-ceph -o jsonpath='{.items[0].metadata.name}')
        mgr_raw=$(kubectl -n rook-ceph exec $toolbox -- ceph mgr dump -f json|jq --raw-output .active_addr)
        timeout 60 sh -c "until kubectl -n rook-ceph exec $toolbox -- curl --silent --show-error ${mgr_raw%%:*}:9283; do echo 'waiting for mgr prometheus exporter to be ready' && sleep 1; done"
        kubectl -n rook-ceph exec $toolbox -- /bin/bash -c "echo \"$(kubectl get pods -o wide -n rook-ceph -l app=rook-ceph-mgr --no-headers=true|head -n1|awk '{print $6"\t"$1}')\" >>/etc/hosts"
        kubectl -n rook-ceph exec $toolbox -- mkdir -p /etc/ceph/test-data
        kubectl -n rook-ceph cp cluster/examples/kubernetes/ceph/test-data/ceph-status-out $toolbox:/etc/ceph/test-data/
        kubectl -n rook-ceph cp cluster/examples/kubernetes/ceph/create-external-cluster-resources.py $toolbox:/etc/ceph
        kubectl -n rook-ceph exec $toolbox -- python3 /etc/ceph/create-external-cluster-resources.py --rbd-data-pool-name replicapool

    - name: run external script create-external-cluster-resources.py unit tests
      run: |
        kubectl -n rook-ceph exec $(kubectl get pod -l app=rook-ceph-tools -n rook-ceph -o jsonpath='{.items[0].metadata.name}') -- python3 -m unittest /etc/ceph/create-external-cluster-resources.py
        # write a test file
        # copy the test file
        # execute the test file

    - name: run kubectl-check-ownerreferences
      run: |
        curl -L https://github.com/kubernetes-sigs/kubectl-check-ownerreferences/releases/download/v0.2.0/kubectl-check-ownerreferences-linux-amd64.tar.gz -o kubectl-check-ownerreferences-linux-amd64.tar.gz
        tar xzvf kubectl-check-ownerreferences-linux-amd64.tar.gz
        chmod +x kubectl-check-ownerreferences
        ./kubectl-check-ownerreferences -n rook-ceph

    - name: Upload canary test result
      uses: actions/upload-artifact@v2
      if: always()
      with:
        name: canary
        path: test

    - name: setup tmate session for debugging
      if: failure() && contains(github.event.pull_request.labels.*.name, 'debug-canary')
      uses: mxschmitt/action-tmate@v3


  pvc:
    runs-on: ubuntu-18.04
    steps:
    - name: checkout
      uses: actions/checkout@v2

    - name: setup golang
      uses: actions/setup-go@v2
      with:
        go-version: 1.16

    - name: install deps
      run: |
        sudo wget https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64 -O /usr/bin/yq
        sudo chmod +x /usr/bin/yq

    - name: setup minikube
      uses: manusa/actions-setup-minikube@v2.3.1
      with:
        minikube version: 'v1.13.1'
        kubernetes version: 'v1.19.2'
        start args: --memory 6g --cpus=2
        github token: ${{ secrets.GITHUB_TOKEN }}

    - name: use local disk and create partitions for osds
      run: |
        BLOCK=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)
        BLOCK_DATA_PART=${BLOCK}1
        sudo dmsetup version||true
        sudo swapoff --all --verbose
        sudo umount /mnt
        # search for the device since it keeps changing between sda and sdb
        sudo wipefs --all --force "$BLOCK_DATA_PART"
        sudo lsblk
        tests/scripts/create-bluestore-partitions.sh --disk "$BLOCK" --osd-count 2
        sudo lsblk

    - name: check k8s cluster status
      run: |
        kubectl cluster-info
        kubectl get pods -n kube-system

    - name: build rook
      run: |
        # set VERSION to a dummy value since Jenkins normally sets it for us. Do this to make Helm happy and not fail with "Error: Invalid Semantic Version"
        GOPATH=$(go env GOPATH) make clean && make -j$nproc IMAGES='ceph' VERSION=0 build
        docker images
        docker tag $(docker images|awk '/build-/ {print $1}') rook/ceph:v1.6.6

    - name: rook prerequisites
      run: |
        BLOCK=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)
        tests/scripts/localPathPV.sh "$BLOCK"
        kubectl create -f cluster/examples/kubernetes/ceph/crds.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/common.yaml

    - name: deploy rook
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/operator.yaml
        yq write -i tests/manifests/test-cluster-on-pvc-encrypted.yaml "spec.storage.storageClassDeviceSets[0].encrypted" false
        yq write -i tests/manifests/test-cluster-on-pvc-encrypted.yaml "spec.storage.storageClassDeviceSets[0].count" 2
        yq write -i tests/manifests/test-cluster-on-pvc-encrypted.yaml "spec.storage.storageClassDeviceSets[0].volumeClaimTemplates[0].spec.resources.requests.storage" 6Gi
        kubectl create -f tests/manifests/test-cluster-on-pvc-encrypted.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/toolbox.yaml

    - name: wait for prepare pod
      run: |
        timeout 180 sh -c '[ $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}'|wc -l) -eq 2 ]; do sleep 5; done'||true
        for prepare in $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}'); do
          kubectl -n rook-ceph logs -f $prepare
          break
        done
        timeout 60 sh -c 'until kubectl -n rook-ceph logs $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd,ceph_daemon_id=0 -o jsonpath='{.items[*].metadata.name}') --all-containers; do echo "waiting for osd container" && sleep 1; done'||true
        kubectl -n rook-ceph describe job/$prepare||true
        kubectl -n rook-ceph describe deploy/rook-ceph-osd-0||true

    - name: wait for ceph to be ready
      run: |
        mkdir test
        tests/scripts/validate_cluster.sh osd 2
        kubectl -n rook-ceph get pods

    - name: run kubectl-check-ownerreferences
      run: |
        curl -L https://github.com/kubernetes-sigs/kubectl-check-ownerreferences/releases/download/v0.2.0/kubectl-check-ownerreferences-linux-amd64.tar.gz -o kubectl-check-ownerreferences-linux-amd64.tar.gz
        tar xzvf kubectl-check-ownerreferences-linux-amd64.tar.gz
        chmod +x kubectl-check-ownerreferences
        ./kubectl-check-ownerreferences -n rook-ceph

    - name: Upload  pvc test result
      uses: actions/upload-artifact@v2
      if: always()
      with:
        name:  pvc
        path: test

    - name: setup tmate session for debugging
      if: failure() && contains(github.event.pull_request.labels.*.name, 'debug-canary-pvc')
      uses: mxschmitt/action-tmate@v3

  pvc-db:
    runs-on: ubuntu-18.04
    steps:
    - name: checkout
      uses: actions/checkout@v2

    - name: setup golang
      uses: actions/setup-go@v2
      with:
        go-version: 1.16

    - name: install deps
      run: |
        sudo wget https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64 -O /usr/bin/yq
        sudo chmod +x /usr/bin/yq
        sudo apt-get install -y gdisk

    - name: setup minikube
      uses: manusa/actions-setup-minikube@v2.3.1
      with:
        minikube version: 'v1.13.1'
        kubernetes version: 'v1.19.2'
        start args: --memory 6g --cpus=2
        github token: ${{ secrets.GITHUB_TOKEN }}

    - name: use local disk
      run: |
        BLOCK_DATA_PART=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)1
        sudo dmsetup version||true
        sudo swapoff --all --verbose
        sudo umount /mnt
        # search for the device since it keeps changing between sda and sdb
        sudo wipefs --all --force "$BLOCK_DATA_PART"
        sudo lsblk

    - name: create bluestore partitions and PVCs
      run: |
        BLOCK=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)
        BLOCK_PART="$BLOCK"2
        DB_PART="$BLOCK"1
        tests/scripts/create-bluestore-partitions.sh --disk "$BLOCK" --bluestore-type block.db --osd-count 1
        tests/scripts/localPathPV.sh "$BLOCK_PART" "$DB_PART"

    - name: check k8s cluster status
      run: |
        kubectl cluster-info
        kubectl get pods -n kube-system

    - name: build rook
      run: |
        # set VERSION to a dummy value since Jenkins normally sets it for us. Do this to make Helm happy and not fail with "Error: Invalid Semantic Version"
        GOPATH=$(go env GOPATH) make clean && make -j$nproc IMAGES='ceph' VERSION=0 build
        docker images
        docker tag $(docker images|awk '/build-/ {print $1}') rook/ceph:v1.6.6

    - name: rook prerequisites
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/crds.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/common.yaml

    - name: deploy rook
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/operator.yaml
        yq write -i tests/manifests/test-cluster-on-pvc-encrypted.yaml "spec.storage.storageClassDeviceSets[0].encrypted" false
        cat tests/manifests/test-on-pvc-db.yaml >> tests/manifests/test-cluster-on-pvc-encrypted.yaml
        kubectl create -f tests/manifests/test-cluster-on-pvc-encrypted.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/toolbox.yaml

    - name: wait for prepare pod
      run: |
        timeout 180 sh -c 'until kubectl -n rook-ceph logs -f $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}'); do sleep 5; done'||true
        timeout 60 sh -c 'until kubectl -n rook-ceph logs $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd,ceph_daemon_id=0 -o jsonpath='{.items[*].metadata.name}') --all-containers; do echo "waiting for osd container" && sleep 1; done'||true
        kubectl -n rook-ceph describe job/$(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}')||true
        kubectl -n rook-ceph describe deploy/rook-ceph-osd-0||true

    - name: wait for ceph to be ready
      run: |
        mkdir test
        tests/scripts/validate_cluster.sh osd 1
        kubectl -n rook-ceph get pods

    - name: Upload pvc-db test result
      uses: actions/upload-artifact@v2
      if: always()
      with:
        name: pvc-db
        path: test

    - name: setup tmate session for debugging
      if: failure() && contains(github.event.pull_request.labels.*.name, 'debug-canary-pvc-db')
      uses: mxschmitt/action-tmate@v3

  pvc-db-wal:
    runs-on: ubuntu-18.04
    steps:
    - name: checkout
      uses: actions/checkout@v2

    - name: setup golang
      uses: actions/setup-go@v2
      with:
        go-version: 1.16

    - name: install deps
      run: |
        sudo wget https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64 -O /usr/bin/yq
        sudo chmod +x /usr/bin/yq
        sudo apt-get install -y gdisk

    - name: setup minikube
      uses: manusa/actions-setup-minikube@v2.3.1
      with:
        minikube version: 'v1.13.1'
        kubernetes version: 'v1.19.2'
        start args: --memory 6g --cpus=2
        github token: ${{ secrets.GITHUB_TOKEN }}

    - name: use local disk
      run: |
        BLOCK_DATA_PART=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)1
        sudo dmsetup version||true
        sudo swapoff --all --verbose
        sudo umount /mnt
        # search for the device since it keeps changing between sda and sdb
        sudo wipefs --all --force "$BLOCK_DATA_PART"
        sudo lsblk

    - name: create bluestore partitions and PVCs
      run: |
        BLOCK=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)
        BLOCK_PART="$BLOCK"3
        DB_PART="$BLOCK"1
        WAL_PART="$BLOCK"2
        tests/scripts/create-bluestore-partitions.sh --disk "$BLOCK" --bluestore-type block.wal --osd-count 1
        tests/scripts/localPathPV.sh "$BLOCK_PART" "$DB_PART" "$WAL_PART"

    - name: check k8s cluster status
      run: |
        kubectl cluster-info
        kubectl get pods -n kube-system

    - name: build rook
      run: |
        # set VERSION to a dummy value since Jenkins normally sets it for us. Do this to make Helm happy and not fail with "Error: Invalid Semantic Version"
        GOPATH=$(go env GOPATH) make clean && make -j$nproc IMAGES='ceph' VERSION=0 build
        docker images
        docker tag $(docker images|awk '/build-/ {print $1}') rook/ceph:v1.6.6

    - name: rook prerequisites
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/crds.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/common.yaml

    - name: deploy rook
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/operator.yaml
        yq write -i tests/manifests/test-cluster-on-pvc-encrypted.yaml "spec.storage.storageClassDeviceSets[0].encrypted" false
        cat tests/manifests/test-on-pvc-db.yaml >> tests/manifests/test-cluster-on-pvc-encrypted.yaml
        cat tests/manifests/test-on-pvc-wal.yaml >> tests/manifests/test-cluster-on-pvc-encrypted.yaml
        kubectl create -f tests/manifests/test-cluster-on-pvc-encrypted.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/toolbox.yaml

    - name: wait for prepare pod
      run: |
        timeout 180 sh -c 'until kubectl -n rook-ceph logs -f $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}'); do sleep 5; done'||true
        timeout 60 sh -c 'until kubectl -n rook-ceph logs $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd,ceph_daemon_id=0 -o jsonpath='{.items[*].metadata.name}') --all-containers; do echo "waiting for osd container" && sleep 1; done'||true
        kubectl -n rook-ceph describe job/$(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}')||true
        kubectl -n rook-ceph describe deploy/rook-ceph-osd-0||true

    - name: wait for ceph to be ready
      run: |
        mkdir test
        tests/scripts/validate_cluster.sh osd 1
        kubectl -n rook-ceph get pods

    - name: Upload pvc-db-wal test result
      uses: actions/upload-artifact@v2
      if: always()
      with:
        name: pvc-db-wal
        path: test

    - name: setup tmate session for debugging
      if: failure() && contains(github.event.pull_request.labels.*.name, 'debug-canary-pvc-db-wal')
      uses: mxschmitt/action-tmate@v3

  encryption-pvc:
    runs-on: ubuntu-18.04
    steps:
    - name: checkout
      uses: actions/checkout@v2

    - name: setup golang
      uses: actions/setup-go@v2
      with:
        go-version: 1.16

    - name: setup minikube
      uses: manusa/actions-setup-minikube@v2.3.1
      with:
        minikube version: 'v1.13.1'
        kubernetes version: 'v1.19.2'
        start args: --memory 6g --cpus=2
        github token: ${{ secrets.GITHUB_TOKEN }}

    - name: check k8s cluster status
      run: |
        kubectl cluster-info
        kubectl get pods -n kube-system

    - name: install yq
      run: |
        sudo wget https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64 -O /usr/bin/yq
        sudo chmod +x /usr/bin/yq

    - name: use local disk and create partitions for osds
      run: |
        BLOCK=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)
        BLOCK_DATA_PART=${BLOCK}1
        sudo dmsetup version||true
        sudo swapoff --all --verbose
        sudo umount /mnt
        # search for the device since it keeps changing between sda and sdb
        sudo wipefs --all --force "$BLOCK_DATA_PART"
        sudo lsblk
        tests/scripts/create-bluestore-partitions.sh --disk "$BLOCK" --osd-count 2
        sudo lsblk

    - name: build rook
      run: |
        # set VERSION to a dummy value since Jenkins normally sets it for us. Do this to make Helm happy and not fail with "Error: Invalid Semantic Version"
        GOPATH=$(go env GOPATH) make clean && make -j$nproc IMAGES='ceph' VERSION=0 build
        docker images
        docker tag $(docker images|awk '/build-/ {print $1}') rook/ceph:v1.6.6

    - name: rook prerequisites
      run: |
        tests/scripts/localPathPV.sh $(lsblk --paths|awk '/14G/ {print $1}'| head -1)
        kubectl create -f cluster/examples/kubernetes/ceph/crds.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/common.yaml

    - name: deploy rook
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/operator.yaml
        yq write -i tests/manifests/test-cluster-on-pvc-encrypted.yaml "spec.storage.storageClassDeviceSets[0].count" 2
        yq write -i tests/manifests/test-cluster-on-pvc-encrypted.yaml "spec.storage.storageClassDeviceSets[0].volumeClaimTemplates[0].spec.resources.requests.storage" 6Gi
        kubectl create -f tests/manifests/test-cluster-on-pvc-encrypted.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/toolbox.yaml

    - name: wait for prepare pod
      run: |
        timeout 180 sh -c 'until kubectl -n rook-ceph logs -f $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}'); do sleep 5; done'||true
        timeout 60 sh -c 'until kubectl -n rook-ceph logs $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd,ceph_daemon_id=0 -o jsonpath='{.items[*].metadata.name}') --all-containers; do echo "waiting for osd container" && sleep 1; done'||true
        kubectl -n rook-ceph describe deploy/rook-ceph-osd-0||true

    - name: wait for ceph to be ready
      run: |
        mkdir test
        tests/scripts/validate_cluster.sh osd 2
        kubectl -n rook-ceph get pods
        kubectl -n rook-ceph get secrets
        sudo lsblk

    - name: Upload encryption-pvc test result
      uses: actions/upload-artifact@v2
      if: always()
      with:
        name: encryption-pvc
        path: test

    - name: setup tmate session for debugging
      if: failure() && contains(github.event.pull_request.labels.*.name, 'debug-encryption-pvc')
      uses: mxschmitt/action-tmate@v3

  encryption-pvc-db:
    runs-on: ubuntu-18.04
    steps:
    - name: checkout
      uses: actions/checkout@v2

    - name: setup golang
      uses: actions/setup-go@v2
      with:
        go-version: 1.16

    - name: install deps
      run: |
        sudo wget https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64 -O /usr/bin/yq
        sudo chmod +x /usr/bin/yq
        sudo apt-get install -y gdisk

    - name: setup minikube
      uses: manusa/actions-setup-minikube@v2.3.1
      with:
        minikube version: 'v1.13.1'
        kubernetes version: 'v1.19.2'
        start args: --memory 6g --cpus=2
        github token: ${{ secrets.GITHUB_TOKEN }}

    - name: use local disk
      run: |
        BLOCK_DATA_PART=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)1
        sudo dmsetup version||true
        sudo swapoff --all --verbose
        sudo umount /mnt
        # search for the device since it keeps changing between sda and sdb
        sudo wipefs --all --force "$BLOCK_DATA_PART"
        sudo lsblk

    - name: create bluestore partitions and PVCs
      run: |
        BLOCK=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)
        BLOCK_PART="$BLOCK"2
        DB_PART="$BLOCK"1
        tests/scripts/create-bluestore-partitions.sh --disk "$BLOCK" --bluestore-type block.db --osd-count 1
        tests/scripts/localPathPV.sh "$BLOCK_PART" "$DB_PART"

    - name: check k8s cluster status
      run: |
        kubectl cluster-info
        kubectl get pods -n kube-system

    - name: build rook
      run: |
        # set VERSION to a dummy value since Jenkins normally sets it for us. Do this to make Helm happy and not fail with "Error: Invalid Semantic Version"
        GOPATH=$(go env GOPATH) make clean && make -j$nproc IMAGES='ceph' VERSION=0 build
        docker images
        docker tag $(docker images|awk '/build-/ {print $1}') rook/ceph:v1.6.6

    - name: rook prerequisites
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/crds.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/common.yaml

    - name: deploy rook
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/operator.yaml
        cat tests/manifests/test-on-pvc-db.yaml >> tests/manifests/test-cluster-on-pvc-encrypted.yaml
        kubectl create -f tests/manifests/test-cluster-on-pvc-encrypted.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/toolbox.yaml

    - name: wait for prepare pod
      run: |
        timeout 180 sh -c 'until kubectl -n rook-ceph logs -f $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}'); do sleep 5; done'||true
        timeout 60 sh -c 'until kubectl -n rook-ceph logs $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd,ceph_daemon_id=0 -o jsonpath='{.items[*].metadata.name}') --all-containers; do echo "waiting for osd container" && sleep 1; done'||true
        kubectl -n rook-ceph describe job/$(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}')||true
        kubectl -n rook-ceph describe deploy/rook-ceph-osd-0||true

    - name: wait for ceph to be ready
      run: |
        mkdir test
        tests/scripts/validate_cluster.sh osd 1
        kubectl -n rook-ceph get pods
        kubectl -n rook-ceph get secrets

    - name: Upload encryption-pvc-db-wal test result
      uses: actions/upload-artifact@v2
      if: always()
      with:
        name: encryption-pvc-db-wal
        path: test

    - name: setup tmate session for debugging
      if: failure() && contains(github.event.pull_request.labels.*.name, 'debug-encryption-pvc-db')
      uses: mxschmitt/action-tmate@v3

  encryption-pvc-db-wal:
    runs-on: ubuntu-18.04
    steps:
    - name: checkout
      uses: actions/checkout@v2

    - name: setup golang
      uses: actions/setup-go@v2
      with:
        go-version: 1.16

    - name: install deps
      run: |
        sudo wget https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64 -O /usr/bin/yq
        sudo chmod +x /usr/bin/yq
        sudo apt-get install -y gdisk

    - name: setup minikube
      uses: manusa/actions-setup-minikube@v2.3.1
      with:
        minikube version: 'v1.13.1'
        kubernetes version: 'v1.19.2'
        start args: --memory 6g --cpus=2
        github token: ${{ secrets.GITHUB_TOKEN }}

    - name: use local disk
      run: |
        BLOCK_DATA_PART=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)1
        sudo dmsetup version||true
        sudo swapoff --all --verbose
        sudo umount /mnt
        # search for the device since it keeps changing between sda and sdb
        sudo wipefs --all --force "$BLOCK_DATA_PART"
        sudo lsblk

    - name: create bluestore partitions and PVCs
      run: |
        BLOCK=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)
        BLOCK_PART="$BLOCK"3
        DB_PART="$BLOCK"1
        WAL_PART="$BLOCK"2
        tests/scripts/create-bluestore-partitions.sh --disk "$BLOCK" --bluestore-type block.wal --osd-count 1
        tests/scripts/localPathPV.sh "$BLOCK_PART" "$DB_PART" "$WAL_PART"

    - name: check k8s cluster status
      run: |
        kubectl cluster-info
        kubectl get pods -n kube-system

    - name: build rook
      run: |
        # set VERSION to a dummy value since Jenkins normally sets it for us. Do this to make Helm happy and not fail with "Error: Invalid Semantic Version"
        GOPATH=$(go env GOPATH) make clean && make -j$nproc IMAGES='ceph' VERSION=0 build
        docker images
        docker tag $(docker images|awk '/build-/ {print $1}') rook/ceph:v1.6.6

    - name: rook prerequisites
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/crds.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/common.yaml

    - name: deploy rook
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/operator.yaml
        cat tests/manifests/test-on-pvc-db.yaml >> tests/manifests/test-cluster-on-pvc-encrypted.yaml
        cat tests/manifests/test-on-pvc-wal.yaml >> tests/manifests/test-cluster-on-pvc-encrypted.yaml
        kubectl create -f tests/manifests/test-cluster-on-pvc-encrypted.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/toolbox.yaml

    - name: wait for prepare pod
      run: |
        timeout 180 sh -c 'until kubectl -n rook-ceph logs -f $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}'); do sleep 5; done'||true
        timeout 60 sh -c 'until kubectl -n rook-ceph logs $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd,ceph_daemon_id=0 -o jsonpath='{.items[*].metadata.name}') --all-containers; do echo "waiting for osd container" && sleep 1; done'||true
        kubectl -n rook-ceph describe job/$(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}')||true
        kubectl -n rook-ceph describe deploy/rook-ceph-osd-0||true

    - name: wait for ceph to be ready
      run: |
        mkdir test
        tests/scripts/validate_cluster.sh osd 1
        kubectl -n rook-ceph get pods
        kubectl -n rook-ceph get secrets

    - name: Upload encryption-pvc-db test result
      uses: actions/upload-artifact@v2
      if: always()
      with:
        name: encryption-pvc-db
        path: test

    - name: setup tmate session for debugging
      if: failure() && contains(github.event.pull_request.labels.*.name, 'debug-encryption-pvc-db-wal')
      uses: mxschmitt/action-tmate@v3

  encryption-pvc-kms-vault-token-auth:
    runs-on: ubuntu-18.04
    steps:
    - name: checkout
      uses: actions/checkout@v2

    - name: setup golang
      uses: actions/setup-go@v2
      with:
        go-version: 1.16

    - name: setup minikube
      uses: manusa/actions-setup-minikube@v2.3.1
      with:
        minikube version: 'v1.13.1'
        kubernetes version: 'v1.19.2'
        start args: --memory 6g --cpus=2
        github token: ${{ secrets.GITHUB_TOKEN }}

    - name: check k8s cluster status
      run: |
        kubectl cluster-info
        kubectl get pods -n kube-system

    - name: install yq
      run: |
        sudo wget https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64 -O /usr/bin/yq
        sudo chmod +x /usr/bin/yq

    - name: use local disk and create partitions for osds
      run: |
        BLOCK=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)
        BLOCK_DATA_PART=${BLOCK}1
        sudo dmsetup version||true
        sudo swapoff --all --verbose
        sudo umount /mnt
        # search for the device since it keeps changing between sda and sdb
        sudo wipefs --all --force "$BLOCK_DATA_PART"
        sudo lsblk
        tests/scripts/create-bluestore-partitions.sh --disk "$BLOCK" --osd-count 2
        sudo lsblk

    - name: build rook
      run: |
        # set VERSION to a dummy value since Jenkins normally sets it for us. Do this to make Helm happy and not fail with "Error: Invalid Semantic Version"
        GOPATH=$(go env GOPATH) make clean && make -j$nproc IMAGES='ceph' VERSION=0 build
        docker images
        docker tag $(docker images|awk '/build-/ {print $1}') rook/ceph:v1.6.6

    - name: rook prereq
      run: |
        tests/scripts/localPathPV.sh $(lsblk --paths|awk '/14G/ {print $1}'| head -1)
        kubectl create -f cluster/examples/kubernetes/ceph/crds.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/common.yaml

    - name: deploy vault
      run: |
        tests/scripts/deploy-validate-vault.sh deploy

    - name: deploy rook
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/operator.yaml
        cat tests/manifests/test-kms-vault.yaml >> tests/manifests/test-cluster-on-pvc-encrypted.yaml
        yq merge --inplace --arrays append tests/manifests/test-cluster-on-pvc-encrypted.yaml tests/manifests/test-kms-vault-spec.yaml
        yq write -i tests/manifests/test-cluster-on-pvc-encrypted.yaml "spec.storage.storageClassDeviceSets[0].count" 2
        yq write -i tests/manifests/test-cluster-on-pvc-encrypted.yaml "spec.storage.storageClassDeviceSets[0].volumeClaimTemplates[0].spec.resources.requests.storage" 6Gi
        kubectl create -f tests/manifests/test-cluster-on-pvc-encrypted.yaml
        yq merge --inplace --arrays append tests/manifests/test-object.yaml tests/manifests/test-kms-vault-spec.yaml
        sed -i 's/ver1/ver2/g' tests/manifests/test-object.yaml
        sed -i 's/VAULT_BACKEND: v1/VAULT_BACKEND: v2/g' tests/manifests/test-object.yaml
        kubectl create -f tests/manifests/test-object.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/toolbox.yaml

    - name: wait for prepare pod
      run: |
        timeout 180 sh -c 'until kubectl -n rook-ceph logs -f $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}'); do sleep 5; done'||true
        timeout 60 sh -c 'until kubectl -n rook-ceph logs $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd,ceph_daemon_id=0 -o jsonpath='{.items[*].metadata.name}') --all-containers; do echo "waiting for osd container" && sleep 1; done'||true
        kubectl -n rook-ceph describe job/$(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}')||true
        kubectl -n rook-ceph describe deploy/rook-ceph-osd-0||true

    - name: wait for ceph to be ready
      run: |
        mkdir test
        tests/scripts/validate_cluster.sh osd 2
        tests/scripts/validate_cluster.sh rgw
        kubectl -n rook-ceph get pods
        kubectl -n rook-ceph get secrets

    - name: validate osd vault
      run: |
        tests/scripts/deploy-validate-vault.sh validate_osd
        sudo lsblk

    - name: validate rgw vault
      run: |
        tests/scripts/deploy-validate-vault.sh validate_rgw

    - name: Upload encryption-pvc-kms-vault-token-auth test result
      uses: actions/upload-artifact@v2
      if: always()
      with:
        name: encryption-pvc-kms-vault-token-auth
        path: test

    - name: setup tmate session for debugging
      if: failure() && contains(github.event.pull_request.labels.*.name, 'debug-encryption-pvc-kms-vault-token-auth')
      uses: mxschmitt/action-tmate@v3

  lvm-pvc:
    runs-on: ubuntu-18.04
    steps:
    - name: checkout
      uses: actions/checkout@v2

    - name: setup golang
      uses: actions/setup-go@v2
      with:
        go-version: 1.16

    - name: install deps
      run: |
        sudo wget https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64 -O /usr/bin/yq
        sudo chmod +x /usr/bin/yq

    - name: setup minikube
      uses: manusa/actions-setup-minikube@v2.3.1
      with:
        minikube version: 'v1.13.1'
        kubernetes version: 'v1.19.2'
        start args: --memory 6g --cpus=2
        github token: ${{ secrets.GITHUB_TOKEN }}

    - name: use local disk
      run: |
        BLOCK_DATA_PART=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)1
        sudo dmsetup version||true
        sudo swapoff --all --verbose
        sudo umount /mnt
        # search for the device since it keeps changing between sda and sdb
        sudo wipefs --all --force "$BLOCK_DATA_PART"
        sudo lsblk

    - name: check k8s cluster status
      run: |
        kubectl cluster-info
        kubectl get pods -n kube-system

    - name: build rook
      run: |
        # set VERSION to a dummy value since Jenkins normally sets it for us. Do this to make Helm happy and not fail with "Error: Invalid Semantic Version"
        GOPATH=$(go env GOPATH) make clean && make -j$nproc IMAGES='ceph' VERSION=0 build
        docker images
        docker tag $(docker images|awk '/build-/ {print $1}') rook/ceph:v1.6.6

    - name: create LV on disk
      run: |
        BLOCK=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)
        sudo sgdisk --zap-all "${BLOCK}"
        VG=test-rook-vg
        LV=test-rook-lv
        sudo pvcreate "$BLOCK"
        sudo vgcreate "$VG" "$BLOCK"
        sudo lvcreate -l 100%FREE -n "${LV}" "${VG}"
        tests/scripts/localPathPV.sh /dev/"${VG}"/${LV}
        kubectl create -f cluster/examples/kubernetes/ceph/crds.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/common.yaml

    - name: deploy rook
      run: |
        kubectl create -f cluster/examples/kubernetes/ceph/operator.yaml
        yq write -i tests/manifests/test-cluster-on-pvc-encrypted.yaml "spec.storage.storageClassDeviceSets[0].encrypted" false
        kubectl create -f tests/manifests/test-cluster-on-pvc-encrypted.yaml
        kubectl create -f cluster/examples/kubernetes/ceph/toolbox.yaml

    - name: wait for prepare pod
      run: |
        timeout 180 sh -c 'until kubectl -n rook-ceph logs -f $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}'); do sleep 5; done'||true
        timeout 60 sh -c 'until kubectl -n rook-ceph logs $(kubectl -n rook-ceph get pod -l app=rook-ceph-osd,ceph_daemon_id=0 -o jsonpath='{.items[*].metadata.name}') --all-containers; do echo "waiting for osd container" && sleep 1; done'||true
        kubectl -n rook-ceph describe job/$(kubectl -n rook-ceph get pod -l app=rook-ceph-osd-prepare -o jsonpath='{.items[*].metadata.name}')||true
        kubectl -n rook-ceph describe deploy/rook-ceph-osd-0||true

    - name: wait for ceph to be ready
      run: |
        mkdir test
        tests/scripts/validate_cluster.sh osd 1
        kubectl -n rook-ceph get pods

    - name: run kubectl-check-ownerreferences
      run: |
        curl -L https://github.com/kubernetes-sigs/kubectl-check-ownerreferences/releases/download/v0.2.0/kubectl-check-ownerreferences-linux-amd64.tar.gz -o kubectl-check-ownerreferences-linux-amd64.tar.gz
        tar xzvf kubectl-check-ownerreferences-linux-amd64.tar.gz
        chmod +x kubectl-check-ownerreferences
        ./kubectl-check-ownerreferences -n rook-ceph

    - name: Upload pvc test result
      uses: actions/upload-artifact@v2
      if: always()
      with:
        name: lvm-pvc
        path: test

    - name: setup tmate session for debugging
      if: failure() && contains(github.event.pull_request.labels.*.name, 'debug-lvm-pvc')
      uses: mxschmitt/action-tmate@v3

  multi-cluster-mirroring:
    runs-on: ubuntu-18.04
    if: "!contains(github.event.pull_request.labels.*.name, 'skip-ci')"
    steps:
    - name: checkout
      uses: actions/checkout@v2

    - name: setup golang
      uses: actions/setup-go@v2
      with:
        go-version: 1.16

    - name: install deps
      run: |
        sudo wget https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64 -O /usr/bin/yq
        sudo chmod +x /usr/bin/yq

    - name: setup minikube
      uses: manusa/actions-setup-minikube@v2.3.1
      with:
        minikube version: 'v1.18.1'
        kubernetes version: 'v1.19.2'
        start args: --memory 6g --cpus=2
        github token: ${{ secrets.GITHUB_TOKEN }}

    - name: use local disk into two partitions
      run: |
        BLOCK=$(sudo lsblk --paths|awk '/14G/ {print $1}'| head -1)
        BLOCK_DATA_PART=${BLOCK}1
        sudo dmsetup version||true
        sudo swapoff --all --verbose
        sudo umount /mnt
        # search for the device since it keeps changing between sda and sdb
        sudo wipefs --all --force "$BLOCK_DATA_PART"
        tests/scripts/create-bluestore-partitions.sh --disk "$BLOCK" --osd-count 2
        sudo lsblk

    - name: build rook
      run: |
        # set VERSION to a dummy value since Jenkins normally sets it for us. Do this to make Helm happy and not fail with "Error: Invalid Semantic Version"
        GOPATH=$(go env GOPATH) make clean && make -j$nproc IMAGES='ceph' VERSION=0 build
        docker images
        docker tag $(docker images|awk '/build-/ {print $1}') rook/ceph:master

    - name: deploy first cluster rook
      run: |
        BLOCK=$(sudo lsblk|awk '/14G/ {print $1}'| head -1)
        cd cluster/examples/kubernetes/ceph/
        kubectl create -f crds.yaml -f common.yaml -f operator.yaml
        yq w -i -d1 cluster-test.yaml spec.dashboard.enabled false
        yq w -i -d1 cluster-test.yaml spec.storage.useAllDevices false
        yq w -i -d1 cluster-test.yaml spec.storage.deviceFilter ${BLOCK}1
        yq w -i -d1 cluster-test.yaml spec.cephVersion.image ceph/daemon-base:latest-pacific-devel
        kubectl create -f cluster-test.yaml -f rbdmirror.yaml -f filesystem-mirror.yaml -f toolbox.yaml

    - name: deploy second cluster rook
      run: |
        BLOCK=$(sudo lsblk|awk '/14G/ {print $1}'| head -1)
        cd cluster/examples/kubernetes/ceph/
        NAMESPACE=rook-ceph-secondary envsubst < common-second-cluster.yaml | kubectl create -f -
        sed -i 's/namespace: rook-ceph/namespace: rook-ceph-secondary/g' cluster-test.yaml rbdmirror.yaml filesystem-mirror.yaml
        yq w -i -d1 cluster-test.yaml spec.storage.deviceFilter ${BLOCK}2
        yq w -i -d1 cluster-test.yaml spec.dataDirHostPath "/var/lib/rook-external"
        yq w -i toolbox.yaml metadata.namespace rook-ceph-secondary
        kubectl create -f cluster-test.yaml -f rbdmirror.yaml -f filesystem-mirror.yaml -f toolbox.yaml

    - name: wait for ceph cluster 1 to be ready
      run: |
        mkdir test
        tests/scripts/validate_cluster.sh osd 1
        kubectl -n rook-ceph get pods

    - name: create replicated mirrored pool on cluster 1
      run: |
        cd cluster/examples/kubernetes/ceph/
        yq w -i pool-test.yaml spec.mirroring.enabled true
        yq w -i pool-test.yaml spec.mirroring.mode image
        kubectl create -f pool-test.yaml
        timeout 60 sh -c 'until [ "$(kubectl -n rook-ceph get cephblockpool replicapool -o jsonpath='{.status.phase}'|grep -c "Ready")" -eq 1 ]; do echo "waiting for pool to created" && sleep 1; done'

    - name: create replicated mirrored pool on cluster 2
      run: |
        cd cluster/examples/kubernetes/ceph/
        yq w -i pool-test.yaml metadata.namespace rook-ceph-secondary
        kubectl create -f pool-test.yaml
        timeout 60 sh -c 'until [ "$(kubectl -n rook-ceph-secondary get cephblockpool replicapool -o jsonpath='{.status.phase}'|grep -c "Ready")" -eq 1 ]; do echo "waiting for pool to created" && sleep 1; done'

    - name: create image in the pool
      run: |
        kubectl exec -n rook-ceph deploy/rook-ceph-tools -ti -- rbd -p replicapool create test -s 1G
        kubectl exec -n rook-ceph deploy/rook-ceph-tools -t -- rbd mirror image enable replicapool/test snapshot
        kubectl exec -n rook-ceph deploy/rook-ceph-tools -t -- rbd -p replicapool info test

    - name: copy peer secret into the other cluster
      run: |
        kubectl -n rook-ceph get secret pool-peer-token-replicapool -o yaml |\
        sed 's/namespace: rook-ceph/namespace: rook-ceph-secondary/g; s/name: pool-peer-token-replicapool/name: pool-peer-token-replicapool-config/g' |\
        kubectl create --namespace=rook-ceph-secondary -f -

    - name: add peer secret to the other cluster
      run: |
        kubectl -n rook-ceph-secondary patch cephrbdmirror my-rbd-mirror --type merge -p '{"spec":{"peers": {"secretNames": ["pool-peer-token-replicapool-config"]}}}'

    - name: verify image has been mirrored
      run: |
        # let's wait a bit for the image to be present
        timeout 30 sh -c 'until [ "$(kubectl exec -n rook-ceph-secondary deploy/rook-ceph-tools -t -- rbd -p replicapool ls|grep -c test)" -eq 1 ]; do echo "waiting for image to be mirrored" && sleep 1; done'

    - name: display cephblockpool and image status
      run: |
        timeout 80 sh -c 'until [ "$(kubectl -n rook-ceph-secondary get cephblockpool replicapool -o jsonpath='{.status.mirroringStatus.summary.daemon_health}'|grep -c OK)" -eq 1 ]; do echo "waiting for mirroring status to be updated" && sleep 1; done'
        kubectl -n rook-ceph-secondary get cephblockpool -o yaml
        kubectl exec -n rook-ceph deploy/rook-ceph-tools -t -- rbd -p replicapool info test

    - name: upload test result
      uses: actions/upload-artifact@v2
      if: always()
      with:
        name: multi-cluster-mirroring
        path: test

    - name: setup tmate session for debugging
      if: failure() && contains(github.event.pull_request.labels.*.name, 'debug-multi-cluster-mirroring')
      uses: mxschmitt/action-tmate@v3
