#################################################################################################################
# Define the settings for the rook-ceph cluster with common settings for a production cluster.
# All nodes with available raw devices will be used for the Ceph cluster. At least three nodes are required
# in this example. See the documentation for more details on storage settings available.

# For example, to create the cluster:
#   kubectl create -f crds.yaml -f common.yaml -f operator.yaml
#   kubectl create -f cluster-with-drive-groups.yaml
#################################################################################################################

apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph # namespace:cluster
spec:
  cephVersion:
      image: ceph/ceph:v15.2.9
      allowUnsupported: false
  skipUpgradeChecks: false
  dataDirHostPath: /var/lib/rook
  mon:
    count: 3
    allowMultiplePerNode: false
  dashboard:
    enabled: true
    ssl: true
  crashCollector:
    disable: false
  annotations:
  labels:
  resources:
  removeOSDsIfOutAndSafeToRemove: false
  # When managing a Rook/Ceph cluster's OSD layouts with Drive Groups, the `storage` config is
  # mostly ignored. `storageClassDeviceSets` can still be used to create OSDs on PVC, but Rook
  # will no longer use `storage` configs for creating OSDs on a node's devices. To avoid
  # confusion, we recommend using the `storage` config OR `driveGroups` and never both.

  # Ceph supports adding devices as OSDs by Ceph Drive Group definitions in Ceph Octopus (v15.2.5+).
  # See Drive Group docs for more info: https://docs.ceph.com/docs/master/cephadm/drivegroups/
  # Drive Groups cannot be used to configure provisioning of OSDs on PVCs.
  # A Drive Group is defined by a name, a Ceph Drive Group spec, and a Rook placement
  driveGroups:
    # Default example: use all nodes and all devices on those nodes
    # This is equivalent to `storage:useAllNodes: true` and `storage:useAllDevices: true`
    - name: all_nodes_all_devices
      spec:
        data_devices:
          all: true
        # The Ceph Drive Group spec's 'placement' and 'host_pattern' fields are ignored in Rook.
        # Rook-based 'placement' below is used instead. The Ceph Drive Group Spec's 'service_id'
        # field is also ignored; it is set by Rook to be equal to the Drive Group name.
        # placement: # is ignored here in favor of Rook's placement below
        # host_pattern: # (deprecated in Ceph) is similarly ignored here in favor of Rook's placement
        # service_id: # is always set to the Drive Group name ("all_nodes" here)
      # omitting 'placement' or leaving it empty matches all untainted nodes
      #placement: # omitting 'placement' or leaving it empty matches all untainted nodes

    # # Further examples
    # - name: nodes_by_hostname
    #   spec:
    #     # example: use rotational devices for data and non-rotational devices for metadata
    #     data_devices:
    #       rotational: 1
    #     db_devices:
    #       rotational: 0
    #   placement:
    #     # example: select nodes by individual names similarly to specifying 'storage:nodes'
    #     nodeAffinity:
    #       requiredDuringSchedulingIgnoredDuringExecution:
    #         nodeSelectorTerms:
    #         - matchExpressions:
    #           - key: kubernetes.io/hostname
    #             operator: In
    #             values:
    #             - k8s-master-0
    #             - k8s-worker-0
    #             - k8s-worker-1
    # - name: nodes_by_label
    #   spec:
    #     # example: max of 6 devices between 10-50TB, and separate db and wal devices
    #     data_devices:
    #       limit: 6
    #       size: "10TB:50TB"
    #     db_devices:
    #       model: SSD-123-foo
    #     wal_devices:
    #       model: NVME-QQQQ-987
    #   placement:
    #     # example: select nodes by label with a toleration
    #     nodeAffinity:
    #       requiredDuringSchedulingIgnoredDuringExecution:
    #         nodeSelectorTerms:
    #         - matchExpressions:
    #           - key: role
    #             operator: In
    #             values:
    #             - storage-node
    #     tolerations:
    #       - key: storage-node
    #         operator: Exists
